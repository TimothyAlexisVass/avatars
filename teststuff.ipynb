{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eib5J7TxG5kY"
      },
      "outputs": [],
      "source": [
        "# Install dependencies.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!rm -r sample_data\n",
        "!pip install -qq --upgrade transformers compel accelerate git+https://github.com/TimothyAlexisVass/diffusers.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fL7u2wgpHilQ"
      },
      "outputs": [],
      "source": [
        "# Set the details for your model here:\n",
        "import torch\n",
        "\n",
        "from diffusers import AutoencoderKL, StableDiffusionXLPipeline, StableDiffusionXLImg2ImgPipeline, \\\n",
        "KDPM2AncestralDiscreteScheduler, DPMSolverSinglestepScheduler, DPMSolverMultistepScheduler, PNDMScheduler, \\\n",
        "DDIMScheduler, LMSDiscreteScheduler, DDPMScheduler, HeunDiscreteScheduler, UniPCMultistepScheduler, \\\n",
        "DEISMultistepScheduler, KDPM2DiscreteScheduler, EulerAncestralDiscreteScheduler, EulerDiscreteScheduler\n",
        "\n",
        "use_refiner = True\n",
        "\n",
        "vae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16)\n",
        "base = StableDiffusionXLPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "    vae=vae,\n",
        "    torch_dtype=torch.float16,\n",
        "    variant=\"fp16\",\n",
        "    use_safetensors=True,\n",
        ")\n",
        "# base.load_lora_weights(\"stabilityai/stable-diffusion-xl-base-1.0\", weight_name=\"sd_xl_offset_example-lora_1.0.safetensors\", alpha=0.5)\n",
        "\n",
        "_ = base.to(\"cuda\")\n",
        "\n",
        "if use_refiner:\n",
        "  refiner = StableDiffusionXLImg2ImgPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-refiner-1.0\",\n",
        "      vae=vae,\n",
        "      text_encoder_2=base.text_encoder_2,\n",
        "      torch_dtype=torch.float16,\n",
        "      variant=\"fp16\",\n",
        "      use_safetensors=True,\n",
        "  )\n",
        "  # refiner.scheduler = scheduler\n",
        "  _ = refiner.to(\"cuda\")\n",
        "\n",
        "tokenizer = base.tokenizer            # cpu\n",
        "tokenizer_2 = base.tokenizer_2        # cpu\n",
        "text_encoder = base.text_encoder      # cuda\n",
        "text_encoder_2 = base.text_encoder_2  # cuda\n",
        "unet = base.unet                      # cuda\n",
        "\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=\"Hillariously obese woman sitting in car laughing hysterically wearing a chewbacca mask.\"\n",
        "negative_prompt = \"bokeh, painting, artwork, blocky, blur, ugly, old, boring, photoshopped, tired, wrinkles, scar, gray hair, big forehead, crosseyed, dumb, stupid, cockeyed, disfigured, blurry, assymetrical, unrealistic, grayscale, bad anatomy, unnatural irises, no pupils, blurry eyes, dark eyes, extra limbs, deformed, disfigured eyes, out of frame, no irises, assymetrical face, broken fingers, extra fingers, disfigured hands\"\n",
        "prompt += \"caricature, colorful, funny, intricate details even to the smallest particle, extreme detail of the environment, sharp portrait, well lit, interesting outfit, beautiful shadows, bright, photoquality, ultra realistic, masterpiece, 8k\"\n",
        "\n",
        "for e in range(1000):\n",
        "  image = base(prompt, negative_prompt=negative_prompt, num_inference_steps=20, denoising_end=0.8, output_type=\"latent\").images\n",
        "  image = refiner(prompt, negative_prompt=negative_prompt, num_inference_steps=20, denoising_start=0.8, image=image).images[0]\n",
        "  image.save(f\"/content/drive/MyDrive/{e}.jpg\")"
      ],
      "metadata": {
        "id": "9rhH5n5TOnX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "import os\n",
        "import time\n",
        "import diffusers\n",
        "from diffusers import PNDMScheduler\n",
        "\n",
        "prompt=\"Photograph of a beautiful woman standing in a lush garden\"\n",
        "negative_prompt = \"bokeh, painting, artwork, blocky, blur, ugly, old, boring, photoshopped, tired, wrinkles, scar, gray hair, big forehead, crosseyed, dumb, stupid, cockeyed, disfigured, blurry, assymetrical, unrealistic, grayscale, bad anatomy, unnatural irises, no pupils, blurry eyes, dark eyes, extra limbs, deformed, disfigured eyes, out of frame, no irises, assymetrical face, broken fingers, extra fingers, disfigured hands\"\n",
        "prompt += \"intricate details even to the smallest particle, extreme detail of the environment, sharp portrait, well lit, interesting outfit, beautiful shadows, bright, photoquality, ultra realistic, masterpiece, 8k\"\n",
        "\n",
        "for use_karras_sigmas in [True]:\n",
        "    for scheduler in base.scheduler.compatibles:\n",
        "        scheduler_name = scheduler.__name__\n",
        "        if scheduler_name in (\"PNDMScheduler\", \"KDPM2AncestralDiscreteScheduler\"):\n",
        "            ske = scheduler.from_config(base.scheduler.config, use_karras_sigmas=use_karras_sigmas)\n",
        "            base.scheduler = ske\n",
        "            for num_inference_steps in range(5, 36, 5):\n",
        "                print(\"Generating with\", scheduler_name)\n",
        "                start_time = time.time()\n",
        "                latents = base(prompt, negative_prompt=negative_prompt, num_inference_steps=num_inference_steps, denoising_end=0.8, output_type=\"latent\").images\n",
        "                image = refiner(prompt, negative_prompt=negative_prompt, num_inference_steps=num_inference_steps, denoising_start=0.8, image=latents).images[0]\n",
        "                folder_path = f\"/content/drive/MyDrive/schedulers/{'k' if use_karras_sigmas else ''}{num_inference_steps}\"\n",
        "                os.makedirs(folder_path, exist_ok=True)\n",
        "                image.save(f\"{folder_path}/{str(round(float(time.time()-start_time),2)).replace('.', ',')}_{scheduler_name}.png\")\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "bXSsr9d-hyF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from compel import Compel, ReturnedEmbeddingsType\n",
        "\n",
        "def process_prompt(pipeline, prompt_1='', negative_prompt_1='', prompt_2='', negative_prompt_2=''):\n",
        "    compel_config = {\n",
        "        \"tokenizer\": [pipeline.tokenizer, pipeline.tokenizer_2],\n",
        "        \"text_encoder\": [pipeline.text_encoder, pipeline.text_encoder_2],\n",
        "        \"requires_pooled\": [False, True],\n",
        "    }\n",
        "    compel_config_1 = {key: value[0] for key, value in compel_config.items()}\n",
        "    compel_config_2 = {key: value[1] for key, value in compel_config.items()}\n",
        "    common_config = {\n",
        "        \"truncate_long_prompts\": False,\n",
        "        \"returned_embeddings_type\": ReturnedEmbeddingsType.PENULTIMATE_HIDDEN_STATES_NON_NORMALIZED,\n",
        "    }\n",
        "    if not pipeline.tokenizer or (prompt_2 == \"\" and negative_prompt_2 == \"\"):\n",
        "        if not pipeline.tokenizer:\n",
        "            compel_config = compel_config_2\n",
        "\n",
        "        compel = Compel(**compel_config, **common_config)\n",
        "        prompt_embeds, prompt_pooled = compel(prompt_1 + prompt_2)\n",
        "        negative_embeds, negative_pooled = compel(negative_prompt_1 + negative_prompt_2)\n",
        "        prompt_embeds, negative_embeds = compel.pad_conditioning_tensors_to_same_length([prompt_embeds, negative_embeds])\n",
        "    else:\n",
        "        compel_1 = Compel(**compel_config_1, **common_config)\n",
        "        compel_2 = Compel(**compel_config_2, **common_config)\n",
        "\n",
        "        prompt_embeds_1 = compel_1(prompt_1)\n",
        "        negative_embeds_1 = compel_1(negative_prompt_1)\n",
        "        prompt_embeds_1, negative_embeds_1 = compel_1.pad_conditioning_tensors_to_same_length([prompt_embeds_1, negative_embeds_1])\n",
        "\n",
        "        prompt_embeds_2, prompt_pooled = compel_2(prompt_2)\n",
        "        negative_embeds_2, negative_pooled = compel_2(negative_prompt_2)\n",
        "        prompt_embeds_2, negative_embeds_2 = compel_2.pad_conditioning_tensors_to_same_length([prompt_embeds_2, negative_embeds_2])\n",
        "\n",
        "        prompt_embeds = torch.cat((prompt_embeds_1, prompt_embeds_2), dim=-1)\n",
        "        negative_embeds = torch.cat((negative_embeds_1, negative_embeds_2), dim=-1)\n",
        "\n",
        "    return {\n",
        "        \"positive\": {\"embeds\": prompt_embeds, \"pooled\": prompt_pooled},\n",
        "        \"negative\": {\"embeds\": negative_embeds, \"pooled\": negative_pooled}\n",
        "    }\n",
        "\n",
        "\n",
        "# Define a function to apply Gaussian blur to a tensor\n",
        "def latents_filter(latent_tensor, timestep, filter_type, kernel_size, sigma, filter_strength):\n",
        "    num_channels = latent_tensor.shape[1]\n",
        "    filtered_latents = []\n",
        "\n",
        "    # Redefine filter_strength based on filter_strength and timestep\n",
        "    timestep_factor = filter_strength * (1.0 - (timestep * 0.001))\n",
        "\n",
        "    for i in range(num_channels):\n",
        "        # Create a Gaussian kernel for blurring\n",
        "        kernel = np.fromfunction(\n",
        "            lambda x, y: (1/ (2 * np.pi * sigma ** 2)) * np.exp(-((x - kernel_size//2)**2 + (y - kernel_size//2)**2) / (2 * sigma**2)),\n",
        "            (kernel_size, kernel_size)\n",
        "        )\n",
        "        kernel = torch.FloatTensor(kernel).to(\"cuda\")\n",
        "        kernel = kernel / kernel.sum()\n",
        "\n",
        "        kernel = kernel.view(1, 1, kernel_size, kernel_size).repeat(1, 1, 1, 1).to(latent_tensor.dtype)\n",
        "        latent_channel = latent_tensor[:, i:i+1, :, :]\n",
        "\n",
        "        filtered_channel = torch.nn.functional.conv2d(latent_channel, kernel, padding=kernel_size//2)\n",
        "\n",
        "        if filter_type == 'sharpen':\n",
        "            # Apply smoothing/sharpening by subtracting the filtered image from the original image\n",
        "            filter_strength = 0.00025 * timestep_factor\n",
        "            filtered_channel = latent_channel + filter_strength * (latent_channel - filtered_channel)\n",
        "        elif filter_type == 'amplify':\n",
        "            # Apply dampen/amplify by applying the filtered image to the original image\n",
        "            filter_strength = 0.002 * timestep_factor\n",
        "            if filter_strength > 0: filter_strength/=3\n",
        "            filtered_channel = latent_channel + filter_strength * filtered_channel\n",
        "        elif filter_type == 'enhance':\n",
        "            # Apply edge enhancement with a Laplacian kernel\n",
        "            filter_strength = 0.00003 * timestep_factor\n",
        "            laplacian_kernel = torch.tensor([[0, 1, 0], [1, -4, 1], [0, 1, 0]], dtype=latent_tensor.dtype).view(1, 1, 3, 3).to(\"cuda\")\n",
        "            edges = torch.nn.functional.conv2d(latent_channel, laplacian_kernel, padding=1)\n",
        "            filtered_channel = latent_channel - filter_strength * edges\n",
        "\n",
        "        filtered_latents.append(filtered_channel)\n",
        "\n",
        "    return torch.cat(filtered_latents, dim=1)\n",
        "\n"
      ],
      "metadata": {
        "id": "PQC_ErCuHaeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2_Hd-BTIBPL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "import diffusers\n",
        "import numpy as np\n",
        "\n",
        "camera_angles = [\n",
        "    # (\"Bird's Eye High altitude Aerial View\", \"Shot from above the subject, providing an aerial perspective.\"), # works\n",
        "    # (\"Angle From Above\", \"Shot from above the subject, looking down.\"), # works\n",
        "    # (\"Eye Level View\", \"Shot taken at the subject's eye level, offering a natural and neutral perspective.\"), # works\n",
        "    # (\"Low Angle View from Below\", \"Shot taken from below the subject, looking up, emphasizing power or dominance.\"), # works\n",
        "    # (\"Dutch Angle Tilted frame View\", \"A slanted or tilted frame to create a sense of unease or disorientation.\"), # doesn't work, but should perhaps be moved anyway\n",
        "    # (\"Aerial drone View\", \"Captured from above using drones or aircraft, offering a unique perspective.\"), # works very well\n",
        "    # (\"Over-the-Shoulder View\", \"Shows the perspective of one character, looking over the shoulder of another.\"), # kinda works\n",
        "    # (\"POV (Point of View) Shot\", \"Replicates what a character or person is seeing, showing their perspective.\"), # works\n",
        "    # (\"3/4 Three quarter angle View\", \"Captures the subject at a three-quarter angle, showing depth and dimension.\"), # kinda works, but should perhaps be in portrait\n",
        "    # (\"Top-Down Viewed directly from above\", \"Shot from directly above, providing an overview of the subject or scene.\"), # kinda works\n",
        "    # (\"Viewed directly from below\", \"Shot from right under, showing the subject from below.\"), # kinda works the same as low angle view from below\n",
        "    (\"Taken Straight On from the front\", \"A shot directly facing the subject, offering a frontal perspective.\"), # doesn't work: \"Straight On Front View\"\n",
        "    # (\"Bilaterally Symmetrical\", \"A view highlighting the subject's bilateral symmetry.\"), # works very well\n",
        "    # (\"Side View\", \"Shot from the side, offering a lateral perspective of the subject.\"), # works\n",
        "    # (\"Back View directly from Behind\", \"Shot from behind the subject, showing their rear perspective.\"), # works very well\n",
        "    # (\"Wide Angle View\", \"A shot using a wide-angle lens, capturing a broader perspective.\"), # works, but should perhaps be in perspectives or shot_sizes\n",
        "    # (\"Hero View\", \"A powerful and dramatic perspective often associated with the hero or protagonist.\"), # doesn't work, but should be in emotions\n",
        "    # (\"Low View\", \"Shot taken from a lower position, emphasizing the subject's dominance or power.\"), Works, but same as Low angle View from below\n",
        "    # (\"Selfie View\", \"A shot taken by the subject themselves, often with a front-facing camera or phone.\"), # works very well\n",
        "]\n",
        "\n",
        "prompt_1 = \"((Dragonball Z)+ Photograph)1.5 In this visually stunning cinematic photo, a towering high mech from the Gundam universe stands proudly amidst a picturesque natural setting (perfect hands)+ in broad daylight with a vast view. The mech is adorned in an intricately designed high-tech mecha armor, primarily white with striking indigo accents that beautifully complement the lush green surroundings. The white mecha armor showcases a level of sophistication and detailing that is truly awe-inspiring. Even the smallest particles on the armor are meticulously crafted, lending an air of authenticity and sophistication to the entire scene. This attention to detail extends to the lush, greenery of the environment as well, with every leaf, blade of grass, and intricate element of the landscape rendered with astonishing precision. The lighting in the image is extraordinary. The natural daylight bathes the scene in a warm and inviting glow, highlighting the Gundam and the surrounding nature park in a way that is both visually captivating and emotionally evocative. The result is a sharp, well-lit portrait that captures the character's sense of happiness and serenity. The outfit is an aesthetic marvel. The combination of white mecha armor and vivid orange details is not just visually striking but also a testament to the designer's creative prowess. The careful interplay of colors, shades, and textures contributes to the overall appeal of the image. Beautiful shadows and contrasts add depth and dimension to the scene, enhancing the realism and vibrancy of the portrayal. The photograph is of high quality and possesses a level of photographic realism that transcends the boundaries of conventional artwork. The overall composition and attention to detail make this portrayal an absolute masterpiece. Rendered in ultra-high definition 8K resolution, this image is a visual masterpiece, captivating the viewer with a level of intricacy and sophistication that is amazing. It is a testament to the skill of photograpy, dedication, and passion for creating a photo that transports the viewer into a galaxy far, far away.\"\n",
        "prompt_1 = \"Forest digital art, (Elven ring)1.4 with intricate design, luminescent, glowing, burning with fire magic, water, intricately detailed, J.R.R. Tolkien's Middle-earth, dark spring forest at night background, in deep shadows, grandiose design, volumetric lighting, strong rim light, radiant light, refraction\"\n",
        "\n",
        "for angle in camera_angles:\n",
        "  angle = angle[0]\n",
        "  prompt_1 = f\"({angle})1.2 Photograph of a beautiful woman standing in a lush garden ({angle})1.3\"\n",
        "\n",
        "  negative_prompt_1 = \"bokeh, painting, artwork, blocky, blur, ugly, old, boring, photoshopped, tired, wrinkles, scar, gray hair, big forehead, crosseyed, dumb, stupid, cockeyed, disfigured, blurry, assymetrical, unrealistic, grayscale, bad anatomy, unnatural irises, no pupils, blurry eyes, dark eyes, extra limbs, deformed, disfigured eyes, out of frame, no irises, assymetrical face, broken fingers, extra fingers, disfigured hands\"\n",
        "  # negative_prompt_1 = \"close-up, visible lightsource, text, watermark, ugly, blurry, overexposed, anime, 3d, worst quality, sketch, deformed, asymmetrical\"\n",
        "  num_inference_steps = 20\n",
        "\n",
        "  prompt_1 += \"intricate details even to the smallest particle, extreme detail of the environment, sharp portrait, well lit, interesting outfit, beautiful shadows, bright, photoquality, ultra realistic, masterpiece, 8k\"\n",
        "\n",
        "  def callback(index, timestep, latents):\n",
        "    if strength != 0:\n",
        "      latents = latents_filter(latents, timestep, 'enhance', kernel_size=7, sigma=2.0, filter_strength=strength)\n",
        "      return {\"latents\": latents}\n",
        "\n",
        "  prompt = process_prompt(base, prompt_1 = prompt_1, negative_prompt_1 = negative_prompt_1)\n",
        "\n",
        "  base_parameters = {\n",
        "      \"prompt_embeds\": prompt['positive']['embeds'],\n",
        "      \"pooled_prompt_embeds\": prompt['positive']['pooled'],\n",
        "      \"negative_prompt_embeds\": prompt['negative']['embeds'],\n",
        "      \"negative_pooled_prompt_embeds\": prompt['negative']['pooled'],\n",
        "      \"num_inference_steps\": num_inference_steps,\n",
        "      \"output_type\": \"latent\",\n",
        "      \"num_images_per_prompt\": 1,\n",
        "      \"guidance_scale\": 8,\n",
        "      \"guidance_rescale\": 0.7,\n",
        "      \"callback\": callback\n",
        "  }\n",
        "  if use_refiner:\n",
        "      prompt = process_prompt(refiner, prompt_1 = prompt_1, negative_prompt_1 = negative_prompt_1)\n",
        "\n",
        "      refiner_parameters = {\n",
        "          \"prompt_embeds\": prompt['positive']['embeds'],\n",
        "          \"pooled_prompt_embeds\": prompt['positive']['pooled'],\n",
        "          \"negative_prompt_embeds\": prompt['negative']['embeds'],\n",
        "          \"negative_pooled_prompt_embeds\": prompt['negative']['pooled'],\n",
        "          \"num_inference_steps\": num_inference_steps, # * 2,\n",
        "          \"output_type\": \"latent\",\n",
        "          \"num_images_per_prompt\": 1,\n",
        "          \"guidance_scale\": 8,\n",
        "      }\n",
        "\n",
        "  height = 1024\n",
        "  width = 1024\n",
        "  # latents = torch.randn((parameters[\"num_images_per_prompt\"], base.unet.config.in_channels, height // 8, width // 8), generator=parameters[\"generator\"]).to(\"cuda\") * base.scheduler.init_noise_sigma\n",
        "\n",
        "  # latents = base.prepare_latents(parameters[\"num_images_per_prompt\"], unet.config.in_channels, height, width, None, base.device, parameters[\"generator\"])\n",
        "\n",
        "  def normalize_tensor(tensor):\n",
        "    min_val = torch.min(tensor)\n",
        "    return (tensor - min_val) / (torch.max(tensor) - min_val)\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def decode_latents(latents, saturation=50, contrast=50, brightness=50, normalize=False):\n",
        "      scaling = 4.444 + saturation / 16\n",
        "\n",
        "      samples = vae.decode(latents * scaling).sample\n",
        "      if normalize:\n",
        "          samples = normalize_tensor(samples)\n",
        "      else:\n",
        "          samples = samples.mul(contrast/100).add(brightness/100).clamp(0, 1)\n",
        "      return base.numpy_to_pil(samples.permute(0, 2, 3, 1).cpu().numpy())\n",
        "  for strength in range(1, 5, 1):\n",
        "      if use_refiner:\n",
        "        wow = base(**base_parameters, generator=torch.manual_seed(2222 + strength), denoising_end=0.8).images\n",
        "        wow = refiner(**refiner_parameters, generator=torch.manual_seed(2222 + strength), denoising_start=0.8, image=wow).images\n",
        "      else:\n",
        "        wow = base(**base_parameters, generator=torch.manual_seed(2222 + strength)).images\n",
        "\n",
        "      image = decode_latents(wow, normalize=False)[0]\n",
        "      print(angle)\n",
        "      display(image)\n",
        "      image.save(f\"/content/drive/MyDrive/angles/{angle}_{strength}.png\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r png_files.zip *.png\n",
        "!rm -r *.png"
      ],
      "metadata": {
        "id": "iPC0I6Aj0qpF"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}