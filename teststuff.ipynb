{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eib5J7TxG5kY"
      },
      "outputs": [],
      "source": [
        "# Install dependencies.\n",
        "!mkdir /root/.ssh\n",
        "!mv /content/id_rsa /root/.ssh/id_rsa\n",
        "!chmod 600 /root/.ssh/id_rsa\n",
        "!ssh-keyscan github.com >> ~/.ssh/known_hosts\n",
        "!git clone git@github.com:TimothyAlexisVass/udi.git\n",
        "!rm -r sample_data\n",
        "!pip install -qq --upgrade transformers accelerate diffusers\n",
        "!mv udi/worker/bilateral_filter.py /content\n",
        "!rm -r sample_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fL7u2wgpHilQ"
      },
      "outputs": [],
      "source": [
        "# Set the details for your model here:\n",
        "import torch\n",
        "\n",
        "from diffusers import StableDiffusionXLPipeline, StableDiffusionXLImg2ImgPipeline, AutoencoderKL, KDPM2AncestralDiscreteScheduler\n",
        "\n",
        "vae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16)\n",
        "base = StableDiffusionXLPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "    vae=vae,\n",
        "    torch_dtype=torch.float16,\n",
        "    variant=\"fp16\",\n",
        "    use_safetensors=True,\n",
        ")\n",
        "\n",
        "base2 = StableDiffusionXLImg2ImgPipeline(**base.components)\n",
        "\n",
        "_ = base.to(\"cuda\")\n",
        "_ = base2.to(\"cuda\")\n",
        "\n",
        "tokenizer = base.tokenizer            # cpu\n",
        "tokenizer_2 = base.tokenizer_2        # cpu\n",
        "scheduler = base.scheduler            # cpu\n",
        "text_encoder = base.text_encoder      # cuda\n",
        "text_encoder_2 = base.text_encoder_2  # cuda\n",
        "unet = base.unet                      # cuda\n",
        "\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(unet.parameters().__next__().device)\n",
        "print(scheduler.sigmas)\n",
        "print(base.device.dtype)"
      ],
      "metadata": {
        "id": "PQC_ErCuHaeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2_Hd-BTIBPL"
      },
      "outputs": [],
      "source": [
        "# @title Default title text\n",
        "import os\n",
        "import zipfile\n",
        "import random\n",
        "import torch\n",
        "import diffusers\n",
        "from bilateral_filter import adaptive_bilateral_filter\n",
        "prompt = f\"Steampunk photo of antropomorphic (suricata), pixar, cute, in gray mecha armor looking happy, daylight lush nature park background. Intricate details even to the smallest particle, extreme detail of the enviroment, sharp portrait, well lit, interesting outfit, beautiful shadows, bright, photoquality, ultra realistic, masterpiece, 8k\"\n",
        "negative_prompt = \"helmet, ugly, old, boring, photoshopped, tired, wrinkles, scar, gray hair, big forehead, crosseyed, dumb, stupid, cockeyed, disfigured, blurry, assymetrical, unrealistic, grayscale, black and white, bald, high hairline, balding, receeding hairline, grayscale, bad anatomy, unnatural irises, no pupils, blurry eyes, dark eyes, extra limbs, deformed, disfigured eyes, out of frame, no irises, assymetrical face, broken fingers, extra fingers, disfigured hands\"\n",
        "num_inference_steps = 20\n",
        "\n",
        "step_fraction = 1/num_inference_steps\n",
        "\n",
        "def callback(s, t, l):\n",
        "  print(t)\n",
        "\n",
        "parameters = {\n",
        "    \"prompt\": prompt,\n",
        "    \"negative_prompt\": negative_prompt,\n",
        "    \"num_inference_steps\": num_inference_steps,\n",
        "    \"output_type\": \"latent\",\n",
        "    \"num_images_per_prompt\": 1,\n",
        "    \"guidance_scale\": 8,\n",
        "    \"denoising_end\": step_fraction * 2,\n",
        "    \"generator\": torch.manual_seed(2222),\n",
        "    \"callback\": callback\n",
        "}\n",
        "\n",
        "import time\n",
        "\n",
        "# Record the start time\n",
        "start_time = time.time()\n",
        "\n",
        "latents = base(**parameters).images\n",
        "\n",
        "height = 1024\n",
        "width = 1024\n",
        "# latents = torch.randn(parameters[\"num_images_per_prompt\"], unet.config.in_channels, (height // 8, width // 8), torch.strided, generator=torch.manual_seed(2222)).to(\"cuda\") * scheduler.init_noise_sigma\n",
        "# latents = base.prepare_latents(parameters[\"num_images_per_prompt\"], unet.config.in_channels, height, width, None, base.device, parameters[\"generator\"])\n",
        "\n",
        "def filter(latents, timestep, sharpness=2):\n",
        "    t = 1.0 - (timestep / 999.0) # You may adjust this based on your needs\n",
        "\n",
        "    # Perform sharpness operation on the latents\n",
        "    alpha = 0.001 * sharpness * t\n",
        "    degraded_latents = adaptive_bilateral_filter(latents)\n",
        "    sharp_latents = degraded_latents * alpha + latents * (1.0 - alpha)\n",
        "\n",
        "    return sharp_latents\n",
        "\n",
        "def normalize_tensor(tensor):\n",
        "  min_val = torch.min(tensor)\n",
        "  return (tensor - min_val) / (torch.max(tensor) - min_val)\n",
        "\n",
        "@torch.no_grad()\n",
        "def decode_latents(latents, saturation=50, contrast=50, brightness=50, normalize=False):\n",
        "    scaling = 4.444 + saturation / 16\n",
        "\n",
        "    samples = vae.decode(latents * scaling).sample\n",
        "    if normalize:\n",
        "        samples = normalize_tensor(samples)\n",
        "    else:\n",
        "        samples = samples.mul(contrast/100).add(brightness/100).clamp(0, 1)\n",
        "    return base.numpy_to_pil(samples.permute(0, 2, 3, 1).cpu().numpy())\n",
        "\n",
        "\n",
        "for step in range(2, num_inference_steps+1, 2):\n",
        "    latents = base2(**parameters, image=latents).images\n",
        "    latents = filter(latents, 1001-step*50, -5)\n",
        "    parameters['denoising_start'] = step * step_fraction\n",
        "    parameters['denoising_end'] += step_fraction * 2\n",
        "\n",
        "images = decode_latents(latents, normalize=True)\n",
        "\n",
        "for image in images:\n",
        "  display(image)\n",
        "\n",
        "print(time.time()-start_time)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Sample tensor\n",
        "tensor = torch.tensor([1.1, 1.0, 0.9, 0.8, -0.2])\n",
        "\n",
        "# Find the minimum and maximum values in the tensor\n",
        "min_val = torch.min(tensor)\n",
        "max_val = torch.max(tensor)\n",
        "\n",
        "# Perform linear normalization\n",
        "normalized_tensor = (tensor - min_val) / (max_val - min_val)\n",
        "\n",
        "print(normalized_tensor)"
      ],
      "metadata": {
        "id": "zkjTq7EX5sP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Default title text\n",
        "import diffusers\n",
        "\n",
        "def model(pipeline, model_type):\n",
        "        if hasattr(diffusers, pipeline):\n",
        "            model_class = getattr(diffusers, pipeline)\n",
        "            return model_class(**base.components)\n",
        "        else:\n",
        "            raise ValueError(f\"Pipeline '{pipeline}' does not exist in Diffusers\")\n",
        "@torch.no_grad()\n",
        "def decode_latents(latents, saturation=50, contrast=50, brightness=50):\n",
        "    scaling = 5 + saturation / 16\n",
        "    print(\"scaling\", scaling)\n",
        "    samples = vae.decode(latents * scaling).sample\n",
        "    samples = samples.mul(contrast/100).add(brightness/100).clamp(0, 1).cpu()\n",
        "    return base.numpy_to_pil(samples.permute(0, 2, 3, 1).numpy())\n",
        "\n",
        "def run_inference(pipeline, model_type, instructions):\n",
        "    global num_inference_steps\n",
        "    num_inference_steps = instructions['num_inference_steps'] = instructions.get('num_inference_steps', 20)\n",
        "\n",
        "    seed = random.randint(1, 2147483647) if instructions.get('seed', 0) == 0 else instructions['seed']\n",
        "    saturation = instructions.get('saturation', 50)\n",
        "    brightness = instructions.get('brightness', 50)\n",
        "    contrast = instructions.get('contrast', 50)\n",
        "\n",
        "    parameters = {\n",
        "        key: instructions[key]\n",
        "        for key in instructions\n",
        "        if key not in [\n",
        "            'seed',\n",
        "            'saturation',\n",
        "            'brightness',\n",
        "            'contrast'\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    parameters['generator'] = torch.manual_seed(seed)\n",
        "    parameters['output_type'] = 'latent'\n",
        "\n",
        "    pipe = model(pipeline, model_type).to('cuda')\n",
        "\n",
        "    latents = pipe(**parameters).images\n",
        "\n",
        "    images = decode_latents(latents, saturation, contrast, brightness)\n",
        "\n",
        "    for index, image in enumerate(images):\n",
        "        display(image)\n",
        "\n",
        "    pipe = None\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "instructions = {\n",
        "    \"prompt\": f\"Steampunk photo of antropomorphic (suricata), pixar, cute, in (gray) mecha armor looking happy, daylight lush nature park background. intricate details even to the smallest particle, extreme detail of the enviroment, sharp portrait, well lit, interesting outfit, beautiful shadows, bright, photoquality, ultra realistic, masterpiece, 8k\",\n",
        "    \"seed\": 2222,\n",
        "    \"negative_prompt\": \"helmet, ugly, old, boring, photoshopped, tired, wrinkles, scar, gray hair, big forehead, crosseyed, dumb, stupid, cockeyed, disfigured, blurry, assymetrical, unrealistic, grayscale, black and white, bald, high hairline, balding, receeding hairline, grayscale, bad anatomy, unnatural irises, no pupils, blurry eyes, dark eyes, extra limbs, deformed, disfigured eyes, out of frame, no irises, assymetrical face, broken fingers, extra fingers, disfigured hands\",\n",
        "    \"guidance_scale\": 9\n",
        "}\n",
        "\n",
        "run_inference(\"StableDiffusionXLPipeline\", \"base\", instructions)"
      ],
      "metadata": {
        "id": "c8GWN1PE0Y92",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iPC0I6Aj0qpF"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}