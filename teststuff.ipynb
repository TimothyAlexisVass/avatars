{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eib5J7TxG5kY"
      },
      "outputs": [],
      "source": [
        "# Install dependencies.\n",
        "!rm -r sample_data\n",
        "!pip install -qq --upgrade transformers compel accelerate git+https://github.com/TimothyAlexisVass/diffusers.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fL7u2wgpHilQ"
      },
      "outputs": [],
      "source": [
        "# Set the details for your model here:\n",
        "import torch\n",
        "\n",
        "from diffusers import AutoencoderKL, StableDiffusionXLPipeline, StableDiffusionXLImg2ImgPipeline, \\\n",
        "KDPM2AncestralDiscreteScheduler, DPMSolverSinglestepScheduler, DPMSolverMultistepScheduler, PNDMScheduler, \\\n",
        "DDIMScheduler, LMSDiscreteScheduler, DDPMScheduler, HeunDiscreteScheduler, UniPCMultistepScheduler, \\\n",
        "DEISMultistepScheduler, KDPM2DiscreteScheduler, EulerAncestralDiscreteScheduler, EulerDiscreteScheduler\n",
        "\n",
        "use_refiner = True\n",
        "\n",
        "vae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16)\n",
        "base = StableDiffusionXLPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "    vae=vae,\n",
        "    torch_dtype=torch.float16,\n",
        "    variant=\"fp16\",\n",
        "    use_safetensors=True,\n",
        ")\n",
        "# base.load_lora_weights(\"stabilityai/stable-diffusion-xl-base-1.0\", weight_name=\"sd_xl_offset_example-lora_1.0.safetensors\", alpha=0.5)\n",
        "\n",
        "# scheduler = PNDMScheduler.from_config(base.scheduler.config, timestep_spacing='trailing', rescale_betas_zero_snr=True, use_karras_sigmas=True) # Set specific Scheduler, defaults: use_karras_sigmas=False\n",
        "# base.scheduler = scheduler\n",
        "_ = base.to(\"cuda\")\n",
        "\n",
        "if use_refiner:\n",
        "  refiner = StableDiffusionXLImg2ImgPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-refiner-1.0\",\n",
        "      vae=vae,\n",
        "      text_encoder_2=base.text_encoder_2,\n",
        "      torch_dtype=torch.float16,\n",
        "      variant=\"fp16\",\n",
        "      use_safetensors=True,\n",
        "  )\n",
        "  refiner.scheduler = scheduler\n",
        "  _ = refiner.to(\"cuda\")\n",
        "\n",
        "tokenizer = base.tokenizer            # cpu\n",
        "tokenizer_2 = base.tokenizer_2        # cpu\n",
        "text_encoder = base.text_encoder      # cuda\n",
        "text_encoder_2 = base.text_encoder_2  # cuda\n",
        "unet = base.unet                      # cuda\n",
        "\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base.scheduler.config\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "PQC_ErCuHaeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2_Hd-BTIBPL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "import diffusers\n",
        "import numpy as np\n",
        "from prompt_manager import process_prompt\n",
        "\n",
        "# Define a function to apply Gaussian blur to a tensor\n",
        "def latents_filter(latent_tensor, timestep, filter_type, kernel_size, sigma, filter_strength):\n",
        "    num_channels = latent_tensor.shape[1]\n",
        "    filtered_latents = []\n",
        "\n",
        "    # Redefine filter_strength based on filter_strength and timestep\n",
        "    timestep_factor = filter_strength * (1.0 - (timestep * 0.001))\n",
        "\n",
        "    for i in range(num_channels):\n",
        "        # Create a Gaussian kernel for blurring\n",
        "        kernel = np.fromfunction(\n",
        "            lambda x, y: (1/ (2 * np.pi * sigma ** 2)) * np.exp(-((x - kernel_size//2)**2 + (y - kernel_size//2)**2) / (2 * sigma**2)),\n",
        "            (kernel_size, kernel_size)\n",
        "        )\n",
        "        kernel = torch.FloatTensor(kernel).to(\"cuda\")\n",
        "        kernel = kernel / kernel.sum()\n",
        "\n",
        "        kernel = kernel.view(1, 1, kernel_size, kernel_size).repeat(1, 1, 1, 1).to(latent_tensor.dtype)\n",
        "        latent_channel = latent_tensor[:, i:i+1, :, :]\n",
        "\n",
        "        filtered_channel = torch.nn.functional.conv2d(latent_channel, kernel, padding=kernel_size//2)\n",
        "\n",
        "        if filter_type == 'sharpen':\n",
        "            # Apply smoothing/sharpening by subtracting the filtered image from the original image\n",
        "            filter_strength = 0.00025 * timestep_factor\n",
        "            filtered_channel = latent_channel + filter_strength * (latent_channel - filtered_channel)\n",
        "        elif filter_type == 'amplify':\n",
        "            # Apply dampen/amplify by applying the filtered image to the original image\n",
        "            filter_strength = 0.002 * timestep_factor\n",
        "            if filter_strength > 0: filter_strength/=3\n",
        "            filtered_channel = latent_channel + filter_strength * filtered_channel\n",
        "        elif filter_type == 'enhance':\n",
        "            # Apply edge enhancement with a Laplacian kernel\n",
        "            filter_strength = 0.00003 * timestep_factor\n",
        "            laplacian_kernel = torch.tensor([[0, 1, 0], [1, -4, 1], [0, 1, 0]], dtype=latent_tensor.dtype).view(1, 1, 3, 3).to(\"cuda\")\n",
        "            edges = torch.nn.functional.conv2d(latent_channel, laplacian_kernel, padding=1)\n",
        "            filtered_channel = latent_channel - filter_strength * edges\n",
        "\n",
        "        filtered_latents.append(filtered_channel)\n",
        "\n",
        "    return torch.cat(filtered_latents, dim=1)\n",
        "\n",
        "camera_angles = [\n",
        "    (\"Bird's Eye View\", \"Shot from directly above the subject, providing an aerial perspective.\"),\n",
        "    (\"High Angle from Above\", \"Shot from above the subject, looking down, creating a sense of vulnerability or inferiority.\"),\n",
        "    (\"Eye Level View\", \"Shot taken at the subject's eye level, offering a natural and neutral perspective.\"),\n",
        "    (\"Low Angle from Below\", \"Shot taken from below the subject, looking up, emphasizing power or dominance.\"),\n",
        "    (\"Dutch Angle Tilted frame View\", \"A slanted or tilted frame to create a sense of unease or disorientation.\"),\n",
        "    (\"Crane View\", \"Taken from a crane or elevated platform to capture dynamic perspectives.\"),\n",
        "    (\"Aerial drone View\", \"Captured from above using drones or aircraft, offering a unique perspective.\"),\n",
        "    (\"Over-the-Shoulder View\", \"Shows the perspective of one character, looking over the shoulder of another.\"),\n",
        "    (\"Over-the-Hip View\", \"Shows the perspective of one character, looking over the shoulder of another.\"),\n",
        "    (\"POV (Point of View) Shot\", \"Replicates what a character or person is seeing, showing their perspective.\"),\n",
        "    (\"3/4 View\", \"Captures the subject at a three-quarter angle, showing depth and dimension.\"),\n",
        "    (\"Top-Down View\", \"Shot from directly above, providing an overview of the subject or scene.\"),\n",
        "    (\"Front Straight On View\", \"A shot directly facing the subject, offering a frontal perspective.\"),\n",
        "    (\"Bilaterally Symmetrical\", \"A view highlighting the subject's bilateral symmetry.\"),\n",
        "    (\"Side View\", \"Shot from the side, offering a lateral perspective of the subject.\"),\n",
        "    (\"Back View from Behind\", \"Shot from behind the subject, showing their rear perspective.\"),\n",
        "    (\"Wide Angle View\", \"A shot using a wide-angle lens, capturing a broader perspective.\"),\n",
        "    (\"Overhead View\", \"A view captured from directly above, similar to a top-down view.\"),\n",
        "    (\"Slightly Above\", \"Shot from a slightly elevated position, creating a perspective slightly higher than eye level.\"),\n",
        "    (\"Hero View\", \"A powerful and dramatic perspective often associated with the hero or protagonist.\"),\n",
        "    (\"Low View\", \"Shot taken from a lower position, emphasizing the subject's dominance or power.\"),\n",
        "    (\"Selfie View\", \"A shot taken by the subject themselves, often with a front-facing camera or phone.\"),\n",
        "]\n",
        "\n",
        "prompt_1 = \"((Dragonball Z)+ Photograph)1.5 In this visually stunning cinematic photo, a towering high mech from the Gundam universe stands proudly amidst a picturesque natural setting (perfect hands)+ in broad daylight with a vast view. The mech is adorned in an intricately designed high-tech mecha armor, primarily white with striking indigo accents that beautifully complement the lush green surroundings. The white mecha armor showcases a level of sophistication and detailing that is truly awe-inspiring. Even the smallest particles on the armor are meticulously crafted, lending an air of authenticity and sophistication to the entire scene. This attention to detail extends to the lush, greenery of the environment as well, with every leaf, blade of grass, and intricate element of the landscape rendered with astonishing precision. The lighting in the image is extraordinary. The natural daylight bathes the scene in a warm and inviting glow, highlighting the Gundam and the surrounding nature park in a way that is both visually captivating and emotionally evocative. The result is a sharp, well-lit portrait that captures the character's sense of happiness and serenity. The outfit is an aesthetic marvel. The combination of white mecha armor and vivid orange details is not just visually striking but also a testament to the designer's creative prowess. The careful interplay of colors, shades, and textures contributes to the overall appeal of the image. Beautiful shadows and contrasts add depth and dimension to the scene, enhancing the realism and vibrancy of the portrayal. The photograph is of high quality and possesses a level of photographic realism that transcends the boundaries of conventional artwork. The overall composition and attention to detail make this portrayal an absolute masterpiece. Rendered in ultra-high definition 8K resolution, this image is a visual masterpiece, captivating the viewer with a level of intricacy and sophistication that is amazing. It is a testament to the skill of photograpy, dedication, and passion for creating a photo that transports the viewer into a galaxy far, far away.\"\n",
        "prompt_1 = \"Forest digital art, (Elven ring)1.4 with intricate design, luminescent, glowing, burning with fire magic, water, intricately detailed, J.R.R. Tolkien's Middle-earth, dark spring forest at night background, in deep shadows, grandiose design, volumetric lighting, strong rim light, radiant light, refraction\"\n",
        "\n",
        "for angle in camera_angles:\n",
        "  angle = angle[0]\n",
        "  prompt_1 = f\"({angle})1.2 Photograph of a beautiful woman\"\n",
        "\n",
        "  negative_prompt_1 = \"bokeh, painting, artwork, blocky, blur, ugly, old, boring, photoshopped, tired, wrinkles, scar, gray hair, big forehead, crosseyed, dumb, stupid, cockeyed, disfigured, blurry, assymetrical, unrealistic, grayscale, bad anatomy, unnatural irises, no pupils, blurry eyes, dark eyes, extra limbs, deformed, disfigured eyes, out of frame, no irises, assymetrical face, broken fingers, extra fingers, disfigured hands\"\n",
        "  # negative_prompt_1 = \"close-up, visible lightsource, text, watermark, ugly, blurry, overexposed, anime, 3d, worst quality, sketch, deformed, asymmetrical\"\n",
        "  num_inference_steps = 20\n",
        "\n",
        "  prompt_1 += \"intricate details even to the smallest particle, extreme detail of the enviroment, sharp portrait, well lit, interesting outfit, beautiful shadows, bright, photoquality, ultra realistic, masterpiece, 8k\"\n",
        "\n",
        "  def callback(index, timestep, latents):\n",
        "    if strength != 0:\n",
        "      latents = latents_filter(latents, timestep, 'enhance', kernel_size=7, sigma=2.0, filter_strength=strength)\n",
        "      return {\"latents\": latents}\n",
        "\n",
        "  prompt = process_prompt(base, prompt_1 = prompt_1, negative_prompt_1 = negative_prompt_1)\n",
        "\n",
        "  base_parameters = {\n",
        "      \"prompt_embeds\": prompt['positive']['embeds'],\n",
        "      \"pooled_prompt_embeds\": prompt['positive']['pooled'],\n",
        "      \"negative_prompt_embeds\": prompt['negative']['embeds'],\n",
        "      \"negative_pooled_prompt_embeds\": prompt['negative']['pooled'],\n",
        "      \"num_inference_steps\": num_inference_steps,\n",
        "      \"output_type\": \"latent\",\n",
        "      \"num_images_per_prompt\": 1,\n",
        "      \"guidance_scale\": 8,\n",
        "      \"guidance_rescale\": 0.7,\n",
        "      \"callback\": callback\n",
        "  }\n",
        "  if use_refiner:\n",
        "      prompt = process_prompt(refiner, prompt_1 = prompt_1, negative_prompt_1 = negative_prompt_1)\n",
        "\n",
        "      refiner_parameters = {\n",
        "          \"prompt_embeds\": prompt['positive']['embeds'],\n",
        "          \"pooled_prompt_embeds\": prompt['positive']['pooled'],\n",
        "          \"negative_prompt_embeds\": prompt['negative']['embeds'],\n",
        "          \"negative_pooled_prompt_embeds\": prompt['negative']['pooled'],\n",
        "          \"num_inference_steps\": num_inference_steps, # * 2,\n",
        "          \"output_type\": \"latent\",\n",
        "          \"num_images_per_prompt\": 1,\n",
        "          \"guidance_scale\": 8,\n",
        "      }\n",
        "\n",
        "  height = 1024\n",
        "  width = 1024\n",
        "  # latents = torch.randn((parameters[\"num_images_per_prompt\"], base.unet.config.in_channels, height // 8, width // 8), generator=parameters[\"generator\"]).to(\"cuda\") * base.scheduler.init_noise_sigma\n",
        "\n",
        "  # latents = base.prepare_latents(parameters[\"num_images_per_prompt\"], unet.config.in_channels, height, width, None, base.device, parameters[\"generator\"])\n",
        "\n",
        "  def normalize_tensor(tensor):\n",
        "    min_val = torch.min(tensor)\n",
        "    return (tensor - min_val) / (torch.max(tensor) - min_val)\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def decode_latents(latents, saturation=50, contrast=50, brightness=50, normalize=False):\n",
        "      scaling = 4.444 + saturation / 16\n",
        "\n",
        "      samples = vae.decode(latents * scaling).sample\n",
        "      if normalize:\n",
        "          samples = normalize_tensor(samples)\n",
        "      else:\n",
        "          samples = samples.mul(contrast/100).add(brightness/100).clamp(0, 1)\n",
        "      return base.numpy_to_pil(samples.permute(0, 2, 3, 1).cpu().numpy())\n",
        "  for strength in range(1, 5, 1):\n",
        "      if use_refiner:\n",
        "        wow = base(**base_parameters, generator=torch.manual_seed(2222 + strength), denoising_end=0.8).images\n",
        "        wow = refiner(**refiner_parameters, generator=torch.manual_seed(2222 + strength), denoising_start=0.8, image=wow).images\n",
        "      else:\n",
        "        wow = base(**base_parameters, generator=torch.manual_seed(2222 + strength)).images\n",
        "\n",
        "      image = decode_latents(wow, normalize=False)[0]\n",
        "      print(angle)\n",
        "      display(image)\n",
        "      image.save(f\"/content/drive/MyDrive/angles/{angle}_{strength}.png\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r png_files.zip *.png\n",
        "!rm -r *.png"
      ],
      "metadata": {
        "id": "iPC0I6Aj0qpF"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}