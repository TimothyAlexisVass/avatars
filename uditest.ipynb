{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!mkdir /root/.ssh\n",
        "!mv /content/id_rsa /root/.ssh/id_rsa\n",
        "!chmod 600 /root/.ssh/id_rsa\n",
        "!ssh-keyscan github.com >> ~/.ssh/known_hosts\n",
        "!git clone git@github.com:TimothyAlexisVass/udi.git\n",
        "!rm -r sample_data"
      ],
      "metadata": {
        "id": "_HZtfK6O3u9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/udi\n",
        "!git pull"
      ],
      "metadata": {
        "id": "wFHgQz_AH0Yu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!lsof -ti:5000 | xargs kill\n",
        "!python3 /content/udi/launch --share"
      ],
      "metadata": {
        "id": "OWWxQJ9n30U9",
        "outputId": "f1e82f34-f405-49ba-d3f8-10a4ec640206",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing accelerate...\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.4/261.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install compel accelerate transformers\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import time\n",
        "import torch\n",
        "import random\n",
        "from udi.worker.prompt_manager import process_prompt\n",
        "from udi.worker.model_manager import ModelManager\n",
        "from udi.worker.filters import latents_filter\n",
        "from PIL.PngImagePlugin import PngInfo\n",
        "\n",
        "model_manager = ModelManager()\n",
        "numpy_to_pil = model_manager.txt2img.numpy_to_pil\n",
        "\n",
        "script_directory = '/content/udi/worker'\n",
        "main_app_directory = os.path.dirname(script_directory)\n",
        "\n",
        "creations_directory = os.path.join(main_app_directory, 'static/creations')\n",
        "preview_directory = os.path.join(main_app_directory, 'static/preview')\n",
        "active_directory = os.path.join(main_app_directory, 'worker/queue/active')\n",
        "done_directory = os.path.join(main_app_directory, 'worker/queue/done')\n",
        "new_directory = os.path.join(main_app_directory, 'worker/queue/new')\n",
        "os.makedirs(creations_directory, exist_ok=True)\n",
        "os.makedirs(preview_directory, exist_ok=True)\n",
        "os.makedirs(active_directory, exist_ok=True)\n",
        "os.makedirs(done_directory, exist_ok=True)\n",
        "os.makedirs(new_directory, exist_ok=True)\n",
        "processing_file = os.path.join(preview_directory, 'processing.json')\n",
        "\n",
        "def console_log(message):\n",
        "    print(str(message) + \"\\n\")\n",
        "def console_error(error):\n",
        "    print(str(error) + \"\\n\")\n",
        "def log_process(data):\n",
        "    with open(processing_file, 'w') as json_file:\n",
        "        json.dump(data, json_file)\n",
        "\n",
        "@torch.no_grad()\n",
        "def decode_latents(latents):\n",
        "    scaling = 4.444 + saturation / 16\n",
        "\n",
        "    samples = model_manager.vae.decode(latents * scaling).sample\n",
        "    if normalize_latents:\n",
        "        samples = normalize_tensor(samples)\n",
        "    else:\n",
        "        samples = samples.mul(contrast/100).add(brightness/100).clamp(0, 1)\n",
        "    return numpy_to_pil(samples.permute(0, 2, 3, 1).cpu().numpy())\n",
        "\n",
        "# Globals\n",
        "num_inference_steps = 20\n",
        "current_step = 1\n",
        "def inference_callback(index, timestep, latents):\n",
        "    global current_step\n",
        "    if timestep > filter_until_timestep:\n",
        "        latents = latents_filter(latents, timestep, filter_type, kernel_size=filter_kernel_size, sigma=filter_spread, filter_strength=filter_strength, filter_until_timestep=filter_until_timestep)\n",
        "    preview = decode_latents(latents)[0]\n",
        "\n",
        "    if current_step < num_inference_steps:\n",
        "        preview.save(os.path.join(preview_directory, f\"{current_step}.jpg\"), quality=70)\n",
        "        log_process({'step': current_step, 'timestep': timestep.item(), 'num_inference_steps': num_inference_steps})\n",
        "        current_step += 1\n",
        "    else:\n",
        "        current_step = 1\n",
        "    previous_file = os.path.join(preview_directory, f\"{current_step - 1}.jpg\")\n",
        "    if os.path.exists(previous_file):\n",
        "        os.remove(previous_file)\n",
        "\n",
        "    console_log(f\"Step: {current_step} / {num_inference_steps} ({timestep.item()})\")\n",
        "\n",
        "    return {\"latents\": latents}\n",
        "\n",
        "pipeline_defaults = {\n",
        "    'num_images_per_prompt': 1,\n",
        "    'output_type': 'latent',\n",
        "    'callback': inference_callback,\n",
        "    'callback_steps': 1,\n",
        "    'guidance_rescale': 0.7,\n",
        "}\n",
        "\n",
        "width_and_height = [\n",
        "    [1536, 640],\n",
        "    [1344, 768],\n",
        "    [1280, 832],\n",
        "    [1152, 896],\n",
        "    [1024, 1024],\n",
        "    [896, 1152],\n",
        "    [832, 1280],\n",
        "    [768, 1344],\n",
        "    [640, 1536],\n",
        "]\n"
      ],
      "metadata": {
        "id": "1kBNiYof5KGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "job_task = {'pipeline': {'prompt': 'A beautiful woman', 'negative_prompt': 'blurry, deformed, disfigured, bad anatomy, ugly', 'resolution': 4, 'guidance_scale': 8.5, 'num_inference_steps': 20, 'seed': 0}, 'inference': {'normalize_latents': False, 'upscale_result': True, 'use_refiner': True, 'saturation': 50, 'brightness': 50, 'contrast': 50, 'filter_type': 'sharpen', 'filter_strength': 5, 'filter_spread': 3, 'filter_kernel_size': 13}, 'scene': {'lighting': 'Low-Key Lighting', 'environment': '', 'ambiance': '', 'composition': '', 'elements': '', 'mood': 'Serene', 'emotion': 'Joy'}, 'view': {'shot_framing': 'Leading Lines Shot', 'focus': '', 'shot_size': 'Close-Up', 'view_angle': '', 'perspective': ''}, 'style': {'artist': '', 'medium': 'Photograph', 'technique': '', 'color_palette': 'Vibrant', 'art_movement': '', 'genre': 'Sci-fi', 'time_period': ''}, 'quality': {'quality_prompt': '', 'word_salad': {'superb': 1, 'photorealistic': 1, 'masterpiece': 1, 'intricate details even to the smallest particle': 1, 'extreme detail of the environment': 1, 'beautifully rendered textures and colors': 1, 'crisp edges': 1, 'high quality': 1}}, 'camera': {'model': 'Nikon D850', 'shutter_speed': '', 'focal_length': 'Ultra-Wide-Angle 20mm', 'aperture': '', 'sensor_format': '', 'sensitivity': '', 'film_format': '', 'film_stock': 'Kodak Portra 400'}}\n",
        "\n",
        "global num_inference_steps\n",
        "\n",
        "###### Define variables ###############################\n",
        "pipeline = job_task[\"pipeline\"]\n",
        "scene = job_task[\"scene\"]\n",
        "view = job_task[\"view\"]\n",
        "camera = job_task[\"camera\"]\n",
        "quality = job_task[\"quality\"]\n",
        "style = job_task[\"style\"]\n",
        "file_name = pipeline[\"prompt\"]\n",
        "\n",
        "num_inference_steps = pipeline[\"num_inference_steps\"]\n",
        "\n",
        "###### Set seed and resolution parameters #############\n",
        "seed = pipeline.get(\"seed\", 0)\n",
        "seed = random.randint(1, 2147483647) if seed == 0 else seed\n",
        "pipeline[\"generator\"] = torch.manual_seed(seed)\n",
        "pipeline[\"width\"], pipeline[\"height\"] = width_and_height[pipeline.get(\"resolution\", 4)]\n",
        "if \"seed\" in pipeline:\n",
        "    del pipeline[\"seed\"]\n",
        "if \"resolution\" in pipeline:\n",
        "    del pipeline[\"resolution\"]\n",
        "\n",
        "console_log(\"Setting global inference parameters\")\n",
        "inference = {}\n",
        "for inference_setting, value in job_task[\"inference\"].items():\n",
        "    globals()[inference_setting] = inference[inference_setting] = value\n",
        "\n",
        "console_log(\"Building prompt...\")\n",
        "###### Build prompt ###################################\n",
        "console_log(\"...scene\")\n",
        "if scene:\n",
        "    pipeline[\"prompt\"] += \" \" + \" \".join(scene.values())\n",
        "\n",
        "console_log(\"...view\")\n",
        "if view:\n",
        "    view_prompt = [view.get(\"shot_size\", \"\"), view.get(\"focus\", \"\"), view.get(\"shot_framing\", \"\")]\n",
        "    if view[\"view_angle\"]:\n",
        "        view_prompt.append(f\"Taken from a {view['view_angle']}\")\n",
        "    if view[\"perspective\"]:\n",
        "        view_prompt.append(f\"with {view['perspective']}\")\n",
        "    pipeline[\"prompt\"] = \"(\" + \" \".join(view_prompt) + \")1.0 \" + pipeline[\"prompt\"]\n",
        "\n",
        "console_log(\"...camera\")\n",
        "if camera:\n",
        "    camera_prompt = []\n",
        "    if camera[\"model\"]:\n",
        "        camera_prompt.append(f\"taken with {camera['model']}\")\n",
        "    camera_prompt.append(camera.get(\"shutter_speed\", \"\"))\n",
        "    camera_prompt.append(camera.get(\"aperture\", \"\"))\n",
        "    camera_prompt.append(camera.get(\"focal_length\", \"\"))\n",
        "    camera_prompt.append(camera.get(\"sensor_format\", \"\"))\n",
        "    camera_prompt.append(camera.get(\"sensitivity\", \"\"))\n",
        "    if camera[\"film_format\"]:\n",
        "        camera_prompt.append(f\"({camera['film_format']} film)1.0\")\n",
        "    camera_prompt.append(camera.get(\"film_format\", \"\"))\n",
        "    if camera[\"focal_length\"]:\n",
        "        camera_prompt.append(f\"({camera['focal_length']} lens)1.0\")\n",
        "    pipeline[\"prompt\"] = \"(\" + \" \".join(camera_prompt) + \")1.0 \" + pipeline[\"prompt\"]\n",
        "\n",
        "console_log(\"...quality\")\n",
        "if quality:\n",
        "    if quality[\"word_salad\"]:\n",
        "        quality_string = \" \".join(f\"({setting}){'{0:.1f}'.format(weight)}\" for setting, weight in quality[\"word_salad\"].items())\n",
        "    else:\n",
        "        quality_string = \"\"\n",
        "    pipeline[\"prompt\"] += f\" {quality['quality_prompt']} \" + quality_string\n",
        "\n",
        "console_log(\"...style\")\n",
        "if style:\n",
        "    if style[\"genre\"]:\n",
        "        pipeline[\"prompt\"] = style[\"genre\"] + \" \" + pipeline[\"prompt\"]\n",
        "    if style[\"medium\"]:\n",
        "        pipeline[\"prompt\"] = style[\"medium\"] + \" \" + pipeline[\"prompt\"]\n",
        "    if style[\"color_palette\"]:\n",
        "        pipeline[\"prompt\"] += f\" {style['color_palette']} colors\"\n",
        "    style_prompt = [style.get(\"artist\", \"\"), style.get(\"technique\", \"\"), style.get(\"art_movement\", \"\"), style.get(\"time_period\", \"\")]\n",
        "    pipeline[\"prompt\"] += \" \" + \" \".join(style_prompt)\n",
        "\n",
        "###### Run Inference ##################################\n",
        "prompt_1 = pipeline.pop(\"prompt\").replace(\"  \", \" \")\n",
        "negative_prompt_1 = pipeline.pop(\"negative_prompt\")\n",
        "\n",
        "\n",
        "console_log(\"Running text to image model\")\n",
        "_ = model_manager.txt2img.to(\"cuda\")\n",
        "prompt_embeds = process_prompt(model_manager.txt2img, prompt_1=prompt_1, negative_prompt_1=negative_prompt_1)\n",
        "prompt_embed_parameters = {\n",
        "    \"prompt_embeds\": prompt_embeds[\"positive\"][\"embeds\"],\n",
        "    \"pooled_prompt_embeds\": prompt_embeds[\"positive\"][\"pooled\"],\n",
        "    \"negative_prompt_embeds\": prompt_embeds[\"negative\"][\"embeds\"],\n",
        "    \"negative_pooled_prompt_embeds\": prompt_embeds[\"negative\"][\"pooled\"]\n",
        "}\n",
        "console_log(\"Setting parameters\")\n",
        "parameters = {**prompt_embed_parameters, **pipeline, **pipeline_defaults}\n",
        "console_log(\"Generating latents\")\n",
        "latents = model_manager.txt2img(**parameters, denoising_end=0.8 if use_refiner else 1.0).images\n",
        "\n",
        "if use_refiner:\n",
        "    console_log(\"Switching to refiner\")\n",
        "    _ = model_manager.refiner.to(\"cuda\")\n",
        "    prompt_embeds = process_prompt(model_manager.refiner, prompt_1=prompt_1, negative_prompt_1=negative_prompt_1)\n",
        "    prompt_embed_parameters = {\n",
        "        \"prompt_embeds\": prompt_embeds[\"positive\"][\"embeds\"],\n",
        "        \"pooled_prompt_embeds\": prompt_embeds[\"positive\"][\"pooled\"],\n",
        "        \"negative_prompt_embeds\": prompt_embeds[\"negative\"][\"embeds\"],\n",
        "        \"negative_pooled_prompt_embeds\": prompt_embeds[\"negative\"][\"pooled\"]\n",
        "    }\n",
        "    console_log(\"Setting parameters\")\n",
        "    parameters = {**prompt_embed_parameters, **pipeline, **pipeline_defaults}\n",
        "    del parameters[\"width\"]\n",
        "    del parameters[\"height\"]\n",
        "    console_log(\"Generating latents\")\n",
        "    latents = model_manager.refiner(**parameters, denoising_start=0.8, image=latents).images\n",
        "\n",
        "console_log(\"Decoding latents\")\n",
        "###### Decode and save image with metadata ############\n",
        "image = decode_latents(latents)[0]\n",
        "\n",
        "del pipeline[\"generator\"]\n",
        "pipeline[\"seed\"] = seed\n",
        "\n",
        "metadata = PngInfo()\n",
        "metadata.add_text(\"pipeline\", json.dumps(job_task[\"pipeline\"]))\n",
        "metadata.add_text(\"scene\", json.dumps(job_task[\"scene\"]))\n",
        "metadata.add_text(\"view\", json.dumps(job_task[\"view\"]))\n",
        "metadata.add_text(\"camera\", json.dumps(job_task[\"camera\"]))\n",
        "metadata.add_text(\"quality\", json.dumps(job_task[\"quality\"]))\n",
        "metadata.add_text(\"style\", json.dumps(job_task[\"style\"]))\n",
        "\n",
        "console_log(\"Saving image\")\n",
        "file_name = f\"{file_name[:100]}_{seed}.png\"\n",
        "image_path = os.path.join(creations_directory, file_name)\n",
        "image.save(image_path, pnginfo=metadata)\n",
        "image.save(os.path.join(preview_directory, f\"{num_inference_steps}.png\"))\n",
        "log_process({\"step\": num_inference_steps, \"timestep\": 1, \"num_inference_steps\": num_inference_steps})\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "console_log(\"Inference completed\")"
      ],
      "metadata": {
        "id": "WMhTIC764v89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(job_task)"
      ],
      "metadata": {
        "id": "617hUE0q6a62"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}